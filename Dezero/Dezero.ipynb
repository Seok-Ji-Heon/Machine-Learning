{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Variable:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.creator = None\n",
    "        \n",
    "    def set_creator(self, func):\n",
    "        self.creator = func\n",
    "        \n",
    "    def backward(self):    \n",
    "        f = self.creator\n",
    "        if f is not None:\n",
    "            x = f.input\n",
    "            x.grad = f.backward(self.grad)\n",
    "            x.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable 및 편의성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    __array_priority = 200\n",
    "    def __init__(self, data, name=None):\n",
    "        if data is not None:\n",
    "            if not isinstance(data, np.ndarray):\n",
    "                raise TypeError('{}은 지원하지 않습니다.'.format(type(data)))\n",
    "                \n",
    "                \n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.grad = None\n",
    "        self.creator = None\n",
    "        self.generation = 0\n",
    "        \n",
    "    def set_creator(self, func):\n",
    "        self.creator = func\n",
    "        self.generation = func.generation + 1\n",
    "        \n",
    "    def backward(self, retain_grad=False, create_graph=False):\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = Variable(np.ones_like(self.data))\n",
    "        \n",
    "        funcs = []\n",
    "        seen_set = set()\n",
    "        \n",
    "        def add_func(f):\n",
    "            if f not in seen_set:\n",
    "                funcs.append(f)\n",
    "                seen_set.add(f)\n",
    "                funcs.sort(key=lambda x: x.generation)\n",
    "        \n",
    "        add_func(self.creator)\n",
    "        \n",
    "        while funcs:\n",
    "            f = funcs.pop()\n",
    "            gys = [output().grad for output in f.outputs]\n",
    "            gxs = f.backward(*gys)\n",
    "            \n",
    "            with using_config('enable_backprop', create_graph):\n",
    "                gxs = f.backward(*gys)\n",
    "                if not isinstance(gxs, tuple):\n",
    "                    gxs = (gxs,)\n",
    "\n",
    "                for x,gx in zip(f.inputs, gxs):\n",
    "                    if x.grad is None:\n",
    "                        x.grad = gx\n",
    "                    else:\n",
    "                        x.grad = x.grad + gx\n",
    "\n",
    "                    if x.creator is not None:\n",
    "                        add_func(x.creator)\n",
    "        \n",
    "        if not retain_grad:\n",
    "            for y in f.outputs:\n",
    "                y().grad = None\n",
    "                    \n",
    "    def reshape(self,*shape):\n",
    "        if len(shape) == 1 and isinstance(shape[0], (tuple,list)):\n",
    "            shape = shape[0]\n",
    "        return reshape(self,shape)\n",
    "    \n",
    "    def transpose(self):\n",
    "        return transpose(self)\n",
    "    \n",
    "    \n",
    "    def cleargrad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def unchain(self):\n",
    "        self.creator = None\n",
    "        \n",
    "    def unchain_backward(self):\n",
    "        if self.creator is not None:\n",
    "            funcs = [self.creator]\n",
    "            while funcs :\n",
    "                f = funcs.pop()\n",
    "                for x in f.inputs:\n",
    "                    if x.creator is not None:\n",
    "                        funcs.append(x.creator)\n",
    "                        x.unchain()\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.data.ndim\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.data.size\n",
    "        \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.data.dtype\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return transpose(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.data is None:\n",
    "            return 'variable(None)'\n",
    "        \n",
    "        p = str(self.data).replace('\\n', '\\n' + ' '*9)\n",
    "        return 'variable('+p +')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Variable):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadcastTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x_shape = x.shape\n",
    "        y = np.broadcast_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        gx = sum_to(gy, self.x_shape)\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTo(Function):\n",
    "    def __init__(self,shape):\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x_shape = x.shape\n",
    "        y = utils_sum_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(Function):\n",
    "    def __init__(self,shape):\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.reshape(self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        return reshape(gy, self.x_shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(Function):\n",
    "    def forward(self,x):\n",
    "        y = np.transpose(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        gx = transpose(gy)\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각종 연산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weakref\n",
    "\n",
    "class Function:\n",
    "    def __call__(self, *inputs):\n",
    "        inputs = [as_variable(x) for x in inputs]\n",
    "        xs = [x.data for x in inputs]\n",
    "        ys = self.forward(*xs)\n",
    "        if not isinstance(ys, tuple):\n",
    "            ys = (ys,)\n",
    "        outputs = [Variable(as_array(y)) for y in ys]\n",
    "        \n",
    "        if Config.enable_backprop:\n",
    "            self.generation = max([x.generation for x in inputs])\n",
    "            for output in outputs:\n",
    "                output.set_creator(self)\n",
    "            self.inputs = inputs\n",
    "            self.outputs = [weakref.ref(output) for output in outputs]\n",
    "        \n",
    "        return outputs if len(outputs) >1 else outputs[0]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    enable_backprop = True\n",
    "    train = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neg(Function):\n",
    "    def forward(self,x):\n",
    "        return -x\n",
    "    \n",
    "    def backward(self,gx):\n",
    "        return -gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    def forward(self,x0,x1):\n",
    "        self.x0_shape, self.x1_shape = x0.shape, x1.shape\n",
    "        y = x0 + x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx0, gx1 = gy, gy\n",
    "        if self.x0_shape != self.x1_shape:\n",
    "            gx0 = sum_to(gx0, self.x0_shape)\n",
    "            gx1 = sum_to(gx1, self.x1_shape)\n",
    "        return gx0, gx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        y = x0 - x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        return gy, -gy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        y = x0 * x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        x0, x1 = self.inputs\n",
    "        return gy*x1, gy*x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Div(Function):\n",
    "    def forward(Self, x0, x1):\n",
    "        y = x0/ x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x0, x1 = self.inputs\n",
    "        gx0 = gy/x1\n",
    "        gx1 = gy*(-x0/ x1 ** 2)\n",
    "        return gx0, gx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pow(Function):\n",
    "    def __init__(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x ** self.c\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        c = self.c\n",
    "        gx = c * x **(c-1) * gy\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(Function):\n",
    "    def forward(self,x):\n",
    "        y = x ** 2\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        gx = 2*x*gy\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(Function):\n",
    "    def forward(self,x):\n",
    "        y = np.exp(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        x, = self.inputs\n",
    "        gx = np.exp(x) * gy\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(Function):\n",
    "    def forward(self,x):\n",
    "        y = np.sin(x)\n",
    "        return y\n",
    "        \n",
    "    def backward(self,gy):\n",
    "        x, = self.inputs\n",
    "        gx = cos(x) * gy\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cos(Function):\n",
    "    def forward(self,x):\n",
    "        y = np.cos(x)\n",
    "        return y\n",
    "        \n",
    "    def backward(self,gy):\n",
    "        x, = self.inputs\n",
    "        gx = -sin(x) * x\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Function):\n",
    "    def forward(self,x):\n",
    "        y = np.tanh(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        y = self.outputs[0]()\n",
    "        gx = gy *(1- y*y)\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(Function):\n",
    "    \n",
    "    def __init__(self,axis, keepdims):\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum(axis = self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        gx = reshape_sum_backward(gy, self.x_shape, self.axis, self.keepdims)\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(Function):\n",
    "    def forward(self,x, W):\n",
    "        y = x.dot(W)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        x, W = self.inputs\n",
    "        gx = matmul(gy, W.T)\n",
    "        gW = matmul(x.T, gy)\n",
    "        return gx, gW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        diff = x0 - x1\n",
    "        y = (diff**2).sum()/len(diff)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        x0, x1 = self.inputs\n",
    "        diff = x0 - x1\n",
    "        gx0 = gy * diff * (2./len(diff))\n",
    "        gx1 = -gx0\n",
    "        return gx0, gx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.exp(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,gy):\n",
    "        gx = gy * y\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetItem(Function):\n",
    "    def __init__(self, slices):\n",
    "        self.slices = slices\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x[self.slices]\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        f = GetItemGrad(self.slices, x.shape)\n",
    "        return f(gy)\n",
    "\n",
    "\n",
    "class GetItemGrad(Function):\n",
    "    def __init__(self, slices, in_shape):\n",
    "        self.slices = slices\n",
    "        self.in_shape = in_shape\n",
    "\n",
    "    def forward(self, gy):\n",
    "        xp = dezero.cuda.get_array_module(gy)\n",
    "        gx = xp.zeros(self.in_shape, dtype=gy.dtype)\n",
    "\n",
    "        if xp is np:\n",
    "            np.add.at(gx, self.slices, gy)\n",
    "        else:\n",
    "            xp.scatter_add(gx, self.slices, gy)\n",
    "        return gx\n",
    "\n",
    "    def backward(self, ggx):\n",
    "        return get_item(ggx, self.slices)\n",
    "\n",
    "\n",
    "def get_item(x, slices):\n",
    "    f = GetItem(slices)\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(x):\n",
    "    return Neg()(x)\n",
    "\n",
    "def square(x):\n",
    "    return Square()(x)\n",
    "\n",
    "def exp(x):\n",
    "    return Exp()(x)\n",
    "\n",
    "def add(x0, x1):\n",
    "    x1 = as_array(x1)\n",
    "    return Add()(x0, x1)\n",
    "\n",
    "def mul(x0, x1):\n",
    "    x1 = as_array(x1)\n",
    "    return Mul()(x0, x1)\n",
    "\n",
    "def sub(x0,x1):\n",
    "    x1 = as_array(x1)\n",
    "    return Sub()(x0, x1)\n",
    "\n",
    "def rsub(x0,x1):\n",
    "    x1 = as_array(x1)\n",
    "    return Sub()(x1,x0)\n",
    "\n",
    "def div(x0, x1):\n",
    "    x1 = as_array(x1)\n",
    "    return Div()(x0,x1)\n",
    "\n",
    "def rdiv(x0, x1):\n",
    "    x1 = as_array(x1)\n",
    "    return Div()(x1,x0)\n",
    "\n",
    "def pow(x, c):\n",
    "    return Pow(c)(x)\n",
    "\n",
    "def sin(x):\n",
    "    return Sin()(x)\n",
    "\n",
    "def cos(x):\n",
    "    return Cos()(x)\n",
    "\n",
    "def tanh(x):\n",
    "    return Tanh()(x)\n",
    "\n",
    "def reshape(x,shape):\n",
    "    if x.shape ==shape:\n",
    "        return as_variable(x)\n",
    "    return Reshape(shape)(x)\n",
    "\n",
    "def transpose(x):\n",
    "    return Transpose()(x)\n",
    "\n",
    "def sum(x, axis=None, keepdims=False):\n",
    "    return Sum(axis, keepdims)(x)\n",
    "\n",
    "def broadcast_to(x, shape):\n",
    "    if x.shape ==shape:\n",
    "        return as_variable(x)\n",
    "    return BroadcastTo(shape)(x)\n",
    "\n",
    "def sum_to(x,shape):\n",
    "    if x.shape==shape:\n",
    "        return as_variable(x)\n",
    "    return SumTo(shape)(x)\n",
    "\n",
    "def matmul(x,W):\n",
    "    return MatMul()(x,W)\n",
    "\n",
    "def mean_squared_error(x0, x1):\n",
    "    return MeanSquaredError()(x0,x1)\n",
    "\n",
    "def exp(x):\n",
    "    return Exp()(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return Sigmoid()(x)\n",
    "\n",
    "def linear(x,W,b=None):\n",
    "    return Linear1()(x,W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "        \n",
    "    def __setattr__(self,name, value):\n",
    "        if isinstance(value, (Parameter, Layer)):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name,value)\n",
    "        \n",
    "    def __call__(self, *inputs):\n",
    "        outputs = self.forward(*inputs)\n",
    "        if not isinstance(outputs, tuple):\n",
    "            outputs = (outputs,)\n",
    "        \n",
    "        self.inputs = [weakref.ref(x) for x in inputs]\n",
    "        self.outputs = [weakref.ref(y) for y in outputs]\n",
    "        return output if len(outputs) > 1 else outputs[0]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            yield self.__dict__[name]\n",
    "            \n",
    "            if isinstance(obj, Layer) or isinstance(obj, Linear):\n",
    "                yield from obj.params()\n",
    "                \n",
    "            else:\n",
    "                yield obj\n",
    "            \n",
    "    def cleargrads(self):\n",
    "        for param in self.params():\n",
    "            param.cleargrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear1(Function):\n",
    "    def forward(self, x, W, b):\n",
    "        y = x.dot(W)\n",
    "        if b is not None:\n",
    "            y += b\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, W, b = self.inputs\n",
    "        gb = None if b.data is None else sum_to(gy, b.shape)\n",
    "        gx = matmul(gy, W.T)\n",
    "        gW = matmul(x.T, gy)\n",
    "        return gx, gW, gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, out_size, nobias=False, dtype = np.float32, in_size=None):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.W = Parameter(None, name='W')\n",
    "        if self.in_size is not None:\n",
    "            self._init_W()\n",
    "        \n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_size, dtype=dtype), name='b')\n",
    "        \n",
    "    def _init_W(self):\n",
    "        I, K = self.in_size, self.out_size\n",
    "        W_data = np.random.randn(I,K).astype(self.dtype) * np.sqrt(1/I)\n",
    "        self.W.data = W_data\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.W.data is None:\n",
    "            self.in_size = x.shape[1]\n",
    "            self._init_W()\n",
    "        \n",
    "        y = linear(x, self.W, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    def __init__(self, fc_output_sizes, activation=sigmoid):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "        \n",
    "        for i, out_size in enumerate(fc_output_sizes):\n",
    "            layer = Linear(out_size)\n",
    "            setattr(self, 'l' + str(i), layer)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    \n",
    "    def forward(self,x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "        \n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "        \n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "            \n",
    "    def update_one(self, param):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "            \n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y/sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_simple(x,t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    p = softmax_simple(x)\n",
    "    p = clip(p, 1e-15, 1.0)\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    def forward(self, x):\n",
    "        y = 1/(1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        y = self.outputs[0]()\n",
    "        gx = gy * y * (1 - y)\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(dataset)\n",
    "        self.max_iter = math.ceil(self.data_size/batch_size)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.iteration =0 \n",
    "        if self.shuffle:\n",
    "            self.index = np.random.permutation(len(self.dataset))\n",
    "        else:\n",
    "            self.index = np.arange(len(self.dataset))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "            \n",
    "        i, batch_size = self.iteration, self.batch_size\n",
    "        batch_index = self.index[i * batch_size: (i+1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "        x = np.array([example[0] for example in batch])\n",
    "        t = np.array([example[1] for example in batch])\n",
    "        \n",
    "        self.iteration +=1\n",
    "        return x, t\n",
    "    \n",
    "    def next(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, gpu=False):\n",
    "        super().__init__(dataset=dataset, batch_size=batch_size, shuffle=False,\n",
    "                         gpu=gpu)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        jump = self.data_size // self.batch_size\n",
    "        batch_index = [(i * jump + self.iteration) % self.data_size for i in\n",
    "                       range(self.batch_size)]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "\n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        x = xp.array([example[0] for example in batch])\n",
    "        t = xp.array([example[1] for example in batch])\n",
    "\n",
    "        self.iteration += 1\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    y, t = as_variable(y), as_variable(t)\n",
    "    \n",
    "    pred = y.data.argmax(axis=1).reshape(t.shape)\n",
    "    result = (pred == t.data)\n",
    "    acc = result.mean()\n",
    "    return Variable(as_array(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x:x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x:x\n",
    "            \n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]), self.target_transform(self.label[index])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigData(Dataset):\n",
    "    def __getitem__(index):\n",
    "        x = np.load('data/{}.npy'.format(index))\n",
    "        t = np.load('label/{}.npy'.format(index))\n",
    "        return x, t\n",
    "    \n",
    "    def __len__():\n",
    "        return 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def my_sin(x, threshold=0.0001):\n",
    "    y = 0\n",
    "    for i in range(100000):\n",
    "        c = (-1)**i/math.factorial(2*i + 1)\n",
    "        t = c*x **(2*i + 1)\n",
    "        y = y + t\n",
    "        if abs(t.data) < threshold:\n",
    "            break\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable.__mul__ = mul\n",
    "Variable.__add__ = add\n",
    "Variable.__radd__ = add\n",
    "Variable.__rmul__ = mul\n",
    "Variable.__neg__ = neg\n",
    "Variable.__sub__ = sub\n",
    "Variable.__rsub__= rsub\n",
    "Variable.__truediv__ = div\n",
    "Variable.__rtruediv__ = rdiv\n",
    "Variable.__pow__ = pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_array(x):\n",
    "    if np.isscalar(x):\n",
    "        return np.array(x)\n",
    "    return x\n",
    "\n",
    "def as_variable(obj):\n",
    "    if isinstance(obj, Variable):\n",
    "        return obj\n",
    "    return Variable(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def using_config(name, value):\n",
    "    old_value = getattr(Config, name)\n",
    "    setattr(Config, name, value)\n",
    "    try:\n",
    "        yield\n",
    "    \n",
    "    finally:\n",
    "        setattr(Config, name, old_value)\n",
    "\n",
    "def test_mode():\n",
    "    return using_config('train', False)\n",
    "        \n",
    "\n",
    "def no_grad():\n",
    "    return using_config('enable_backprop', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sum_backward(gy, x_shape, axis, keepdims):\n",
    "    \"\"\"Reshape gradient appropriately for dezero.functions.sum's backward.\n",
    "    Args:\n",
    "        gy (dezero.Variable): Gradient variable from the output by backprop.\n",
    "        x_shape (tuple): Shape used at sum function's forward.\n",
    "        axis (None or int or tuple of ints): Axis used at sum function's\n",
    "            forward.\n",
    "        keepdims (bool): Keepdims used at sum function's forward.\n",
    "    Returns:\n",
    "        dezero.Variable: Gradient variable which is reshaped appropriately\n",
    "    \"\"\"\n",
    "    ndim = len(x_shape)\n",
    "    tupled_axis = axis\n",
    "    if axis is None:\n",
    "        tupled_axis = None\n",
    "    elif not isinstance(axis, tuple):\n",
    "        tupled_axis = (axis,)\n",
    "\n",
    "    if not (ndim == 0 or tupled_axis is None or keepdims):\n",
    "        actual_axis = [a if a >= 0 else a + ndim for a in tupled_axis]\n",
    "        shape = list(gy.shape)\n",
    "        for a in sorted(actual_axis):\n",
    "            shape.insert(a, 1)\n",
    "    else:\n",
    "        shape = gy.shape\n",
    "\n",
    "    gy = gy.reshape(shape)  # reshape\n",
    "    return gy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_sum_to(x, shape):\n",
    "    \"\"\"Sum elements along axes to output an array of a given shape.\n",
    "    Args:\n",
    "        x (ndarray): Input array.\n",
    "        shape:\n",
    "    Returns:\n",
    "        ndarray: Output array of the shape.\n",
    "    \"\"\"\n",
    "    ndim = len(shape)\n",
    "    lead = x.ndim - ndim\n",
    "    lead_axis = tuple(range(lead))\n",
    "\n",
    "    axis = tuple([i + lead for i, sx in enumerate(shape) if sx == 1])\n",
    "    y = x.sum(lead_axis + axis, keepdims=True)\n",
    "    if lead > 0:\n",
    "        y = y.squeeze(lead_axis)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, dropout_ratio = 0.5):\n",
    "    x = as_variable(x)\n",
    "    \n",
    "    if Config.train:\n",
    "        mask = np.random.rand(*x.shape)> dropout_ratio\n",
    "        scale = np.array(1.0 - dropout_ratio).astype(x.dtype)\n",
    "        y = x * mask/ scale\n",
    "        return y\n",
    "    \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Im2col(Function):\n",
    "    def __init__(self, kernel_size, stride, pad, to_matrix):\n",
    "        super().__init__()\n",
    "        self.input_shape = None\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.to_matrix = to_matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        y = im2col_array(x, self.kernel_size, self.stride, self.pad,\n",
    "                         self.to_matrix)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        gx = col2im(gy, self.input_shape, self.kernel_size, self.stride,\n",
    "                    self.pad, self.to_matrix)\n",
    "        return gx\n",
    "\n",
    "\n",
    "def im2col(x, kernel_size, stride=1, pad=0, to_matrix=True):\n",
    "    \"\"\"Extract patches from an image based on the filter.\n",
    "    Args:\n",
    "        x (`dezero.Variable` or `ndarray`): Input variable of shape\n",
    "            `(N, C, H, W)`\n",
    "        kernel_size (int or (int, int)): Size of kernel.\n",
    "        stride (int or (int, int)): Stride of kernel.\n",
    "        pad (int or (int, int)): Spatial padding width for input arrays.\n",
    "        to_matrix (bool): If True the `col` will be reshaped to 2d array whose\n",
    "            shape is `(N*OH*OW, C*KH*KW)`\n",
    "    Returns:\n",
    "        `dezero.Variable`: Output variable. If the `to_matrix` is False, the\n",
    "            output shape is `(N, C, KH, KW, OH, OW)`, otherwise\n",
    "            `(N*OH*OW, C*KH*KW)`.\n",
    "    Notation:\n",
    "    - `N` is the batch size.\n",
    "    - `C` is the number of the input channels.\n",
    "    - `H` and `W` are the height and width of the input image, respectively.\n",
    "    - `KH` and `KW` are the height and width of the filters, respectively.\n",
    "    - `SH` and `SW` are the strides of the filter.\n",
    "    - `PH` and `PW` are the spatial padding sizes.\n",
    "    - `OH` and `OW` are the the height and width of the output, respectively.\n",
    "    \"\"\"\n",
    "    y = Im2col(kernel_size, stride, pad, to_matrix)(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_outsize(input_size, kernel_size, stride, pad):\n",
    "    return (input_size + pad * 2 - kernel_size) // stride + 1\n",
    "\n",
    "\n",
    "def pair(x):\n",
    "    if isinstance(x, int):\n",
    "        return (x, x)\n",
    "    elif isinstance(x, tuple):\n",
    "        assert len(x) == 2\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(Function):\n",
    "    def __init__(self, stride=1, pad=0):\n",
    "        super().__init__()\n",
    "        self.stride = pair(stride)\n",
    "        self.pad = pair(pad)\n",
    "\n",
    "    def forward(self, x, W, b):\n",
    "        xp = cuda.get_array_module(x)\n",
    "\n",
    "        KH, KW = W.shape[2:]\n",
    "        col = im2col_array(x, (KH, KW), self.stride, self.pad, to_matrix=False)\n",
    "\n",
    "        y = xp.tensordot(col, W, ((1, 2, 3), (1, 2, 3)))\n",
    "        if b is not None:\n",
    "            y += b\n",
    "        y = xp.rollaxis(y, 3, 1)\n",
    "        # y = np.transpose(y, (0, 3, 1, 2))\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, W, b = self.inputs\n",
    "        # ==== gx ====\n",
    "        gx = deconv2d(gy, W, b=None, stride=self.stride, pad=self.pad,\n",
    "                      outsize=(x.shape[2], x.shape[3]))\n",
    "        # ==== gW ====\n",
    "        gW = Conv2DGradW(self)(x, gy)\n",
    "        # ==== gb ====\n",
    "        gb = None\n",
    "        if b.data is not None:\n",
    "            gb = gy.sum(axis=(0, 2, 3))\n",
    "        return gx, gW, gb\n",
    "\n",
    "\n",
    "def conv2d(x, W, b=None, stride=1, pad=0):\n",
    "    return Conv2d(stride, pad)(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_simple(x, kernel_size, stride=1, pad=0):\n",
    "    x = as_variable(x)\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    KH, KW = pair(kernel_size)\n",
    "    PH, PW = pair(pad)\n",
    "    SH, SW = pair(stride)\n",
    "    OH = get_conv_outsize(H, KH, SH, PH)\n",
    "    OW = get_conv_outsize(W, KW, SW, PW)\n",
    "\n",
    "    col = im2col(x, kernel_size, stride, pad, to_matrix=True)\n",
    "    col = col.reshape(-1, KH * KW)\n",
    "    y = col.max(axis=1)\n",
    "    y = y.reshape(N, OH, OW, C).transpose(0, 3, 1, 2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Layer):\n",
    "    def __init__(self, hidden_size, in_size=None):\n",
    "        \"\"\"An Elman RNN with tanh.\n",
    "        Args:\n",
    "            hidden_size (int): The number of features in the hidden state.\n",
    "            in_size (int): The number of features in the input. If unspecified\n",
    "            or `None`, parameter initialization will be deferred until the\n",
    "            first `__call__(x)` at which time the size will be determined.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.x2h = Linear(hidden_size, in_size=in_size)\n",
    "        self.h2h = Linear(hidden_size, in_size=in_size, nobias=True)\n",
    "        self.h = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.h is None:\n",
    "            h_new = F.tanh(self.x2h(x))\n",
    "        else:\n",
    "            h_new = F.tanh(self.x2h(x) + self.h2h(self.h))\n",
    "        self.h = h_new\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(Layer):\n",
    "    def __init__(self, hidden_size, in_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        H, I = hidden_size, in_size\n",
    "        self.x2f = Linear(H, in_size=I)\n",
    "        self.x2i = Linear(H, in_size=I)\n",
    "        self.x2o = Linear(H, in_size=I)\n",
    "        self.x2u = Linear(H, in_size=I)\n",
    "        self.h2f = Linear(H, in_size=H, nobias=True)\n",
    "        self.h2i = Linear(H, in_size=H, nobias=True)\n",
    "        self.h2o = Linear(H, in_size=H, nobias=True)\n",
    "        self.h2u = Linear(H, in_size=H, nobias=True)\n",
    "        self.reset_state()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.h is None:\n",
    "            f = F.sigmoid(self.x2f(x))\n",
    "            i = F.sigmoid(self.x2i(x))\n",
    "            o = F.sigmoid(self.x2o(x))\n",
    "            u = F.tanh(self.x2u(x))\n",
    "        else:\n",
    "            f = F.sigmoid(self.x2f(x) + self.h2f(self.h))\n",
    "            i = F.sigmoid(self.x2i(x) + self.h2i(self.h))\n",
    "            o = F.sigmoid(self.x2o(x) + self.h2o(self.h))\n",
    "            u = F.tanh(self.x2u(x) + self.h2u(self.h))\n",
    "\n",
    "        if self.c is None:\n",
    "            c_new = (i * u)\n",
    "        else:\n",
    "            c_new = (f * self.c) + (i * u)\n",
    "\n",
    "        h_new = o * F.tanh(c_new)\n",
    "\n",
    "        self.h, self.c = h_new, c_new\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "variable(2.0)\n",
      "variable(1.0)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(2.0))\n",
    "y = add(x, x)\n",
    "print(y.data)\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n",
      "variable(64.0)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(2.0))\n",
    "a = square(x)\n",
    "y = add(square(a),square(a))\n",
    "y.backward()\n",
    "\n",
    "print(y.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(2.0) variable(2.0)\n"
     ]
    }
   ],
   "source": [
    "def sphere(x,y):\n",
    "    z = x**2 + y**2\n",
    "    return z\n",
    "\n",
    "x = Variable(np.array(1.0))\n",
    "y = Variable(np.array(1.0))\n",
    "z = sphere(x, y)\n",
    "z.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.040000000000000036) variable(0.040000000000000036)\n"
     ]
    }
   ],
   "source": [
    "def matyas(x, y):\n",
    "    z = 0.26 * (x **2 + y **2) - 0.48 * x * y\n",
    "    return z\n",
    "\n",
    "x = Variable(np.array(1.0))\n",
    "y = Variable(np.array(1.0))\n",
    "z = matyas(x, y)\n",
    "z.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(-5376.0) variable(8064.0)\n"
     ]
    }
   ],
   "source": [
    "def goldstein(x, y):\n",
    "    z = (1 + (x + y + 1)**2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y**2)) * \\\n",
    "        (30 + (2*x - 3*y)**2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y**2))\n",
    "    return z\n",
    "\n",
    "\n",
    "x = Variable(np.array(1.0))\n",
    "y = Variable(np.array(1.0))\n",
    "z = goldstein(x, y) \n",
    "z.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8660254037844386\n",
      "variable(0.5000000000000001)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(np.pi/3))\n",
    "y = sin(x)\n",
    "y.backward()\n",
    "\n",
    "print(y.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071064695751781\n",
      "variable(0.7071032148228457)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(np.pi/4))\n",
    "y = my_sin(x)\n",
    "y.backward()\n",
    "\n",
    "print(y.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(24.0)\n",
      "variable(44.0)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = x **4 - 2*x**2\n",
    "    return y\n",
    "\n",
    "x = Variable(np.array(2.0))\n",
    "y = f(x)\n",
    "y.backward(create_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "gx = x.grad\n",
    "x.cleargrad()\n",
    "gx.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 variable(2.0)\n",
      "1 variable(1.4545454545454546)\n",
      "2 variable(1.1510467893775467)\n",
      "3 variable(1.0253259289766978)\n",
      "4 variable(1.0009084519430513)\n",
      "5 variable(1.0000012353089454)\n",
      "6 variable(1.000000000002289)\n",
      "7 variable(1.0)\n",
      "8 variable(1.0)\n",
      "9 variable(1.0)\n"
     ]
    }
   ],
   "source": [
    "iters = 10\n",
    "\n",
    "for i in range(iters):\n",
    "    print(i,x)\n",
    "    \n",
    "    y = f(x)\n",
    "    x.cleargrad()\n",
    "    y.backward(create_graph=True)\n",
    "    \n",
    "    gx = x.grad\n",
    "    x.cleargrad()\n",
    "    gx.backward()\n",
    "    gx2 = x.grad\n",
    "    \n",
    "    x.data -=gx.data/gx2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(-0.8414709848078965)\n",
      "variable(-1.3817732906760363)\n",
      "variable(-2.4623779024123156)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(1.0))\n",
    "y = sin(x)\n",
    "y.backward(create_graph=True)\n",
    "\n",
    "for i in range(3):\n",
    "    gx = x.grad\n",
    "    x.cleargrad()\n",
    "    gx.backward(create_graph=True)\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = reshape(x,(6,))\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[ 1.66323476  1.0134988   1.13236135]\n",
       "          [-1.34233188  0.54592552  0.3996601 ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(np.random.randn(1,2,3))\n",
    "y = x.reshape((2,3))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.reshape(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[ 1.66323476  1.0134988   1.13236135]\n",
       "          [-1.34233188  0.54592552  0.3996601 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "variable([[1 1 1]\n",
       "          [1 1 1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = sum(x,axis=0)\n",
    "print(y.backward())\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([11 12 13])\n",
      "variable([3])\n"
     ]
    }
   ],
   "source": [
    "x0 = Variable(np.array([1,2,3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 + x1\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.randn(2,3))\n",
    "W = Variable(np.random.randn(3,4))\n",
    "y = matmul(x,W)\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.shape)\n",
    "print(W.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.64433458]]) variable([1.29473389]) variable(42.296340129442335)\n",
      "variable([[1.12672345]]) variable([2.26959351]) variable(23.97380754378544)\n",
      "variable([[1.48734571]]) variable([3.00386712]) variable(13.609686745040522)\n",
      "variable([[1.75641886]]) variable([3.557186]) variable(7.747049961219976)\n",
      "variable([[1.95666851]]) variable([3.97439789]) variable(4.43057410592155)\n",
      "variable([[2.10518573]]) variable([4.28923203]) variable(2.554280381353593)\n",
      "variable([[2.21482401]]) variable([4.52705574]) variable(1.492599869047195)\n",
      "variable([[2.29524981]]) variable([4.70694745]) variable(0.8916952181756939)\n",
      "variable([[2.35373273]]) variable([4.84325585]) variable(0.5514270962227455)\n",
      "variable([[2.39573972]]) variable([4.9467725]) variable(0.3585915308319281)\n",
      "variable([[2.425382]]) variable([5.02561369]) variable(0.24915731977561134)\n",
      "variable([[2.44575118]]) variable([5.08588371]) variable(0.1869065876539789)\n",
      "variable([[2.45917205]]) variable([5.13217364]) variable(0.1513533629631488)\n",
      "variable([[2.4673927]]) variable([5.16793652]) variable(0.13091003006317087)\n",
      "variable([[2.47172747]]) variable([5.19576949]) variable(0.11902210735018467)\n",
      "variable([[2.47316455]]) variable([5.21762597]) variable(0.11198198322254362)\n",
      "variable([[2.47244676]]) variable([5.23497527]) variable(0.10769231158094321)\n",
      "variable([[2.47013247]]) variable([5.24892259]) variable(0.10496655795675108)\n",
      "variable([[2.46664127]]) variable([5.26029927]) variable(0.10313337115761934)\n",
      "variable([[2.46228843]]) variable([5.26973075]) variable(0.10181280604960243)\n",
      "variable([[2.45731071]]) variable([5.27768752]) variable(0.10078974954301652)\n",
      "variable([[2.4518859]]) variable([5.28452363]) variable(0.09994232708821599)\n",
      "variable([[2.44614738]]) variable([5.29050548]) variable(0.09920140749444821)\n",
      "variable([[2.44019517]]) variable([5.29583359]) variable(0.09852769772358984)\n",
      "variable([[2.4341042]]) variable([5.30065891]) variable(0.09789878700703991)\n",
      "variable([[2.4279305]]) variable([5.30509512]) variable(0.09730181854646197)\n",
      "variable([[2.42171596]]) variable([5.30922787]) variable(0.0967293443169877)\n",
      "variable([[2.41549177]]) variable([5.3131217]) variable(0.09617698031441604)\n",
      "variable([[2.40928112]]) variable([5.31682532]) variable(0.09564208018092028)\n",
      "variable([[2.40310116]]) variable([5.32037549]) variable(0.09512298485383605)\n",
      "variable([[2.39696452]]) variable([5.3238]) variable(0.09461859803040694)\n",
      "variable([[2.39088043]]) variable([5.32711987]) variable(0.09412814592514404)\n",
      "variable([[2.38485555]]) variable([5.33035108]) variable(0.09365104127065577)\n",
      "variable([[2.37889464]]) variable([5.33350575]) variable(0.09318680628411545)\n",
      "variable([[2.37300101]]) variable([5.33659314]) variable(0.09273502898904006)\n",
      "variable([[2.3671769]]) variable([5.33962035]) variable(0.09229533840647547)\n",
      "variable([[2.36142374]]) variable([5.34259283]) variable(0.09186739042193233)\n",
      "variable([[2.35574235]]) variable([5.34551483]) variable(0.09145085969346782)\n",
      "variable([[2.35013309]]) variable([5.34838966]) variable(0.09104543497939387)\n",
      "variable([[2.34459602]]) variable([5.35121993]) variable(0.09065081640275062)\n",
      "variable([[2.33913091]]) variable([5.35400772]) variable(0.0902667138137311)\n",
      "variable([[2.33373736]]) variable([5.35675472]) variable(0.08989284577554084)\n",
      "variable([[2.32841486]]) variable([5.35946234]) variable(0.0895289389052284)\n",
      "variable([[2.32316275]]) variable([5.36213172]) variable(0.08917472741757472)\n",
      "variable([[2.31798034]]) variable([5.36476385]) variable(0.08882995278605695)\n",
      "variable([[2.31286689]]) variable([5.3673596]) variable(0.08849436347219049)\n",
      "variable([[2.30782159]]) variable([5.36991973]) variable(0.08816771469564999)\n",
      "variable([[2.30284363]]) variable([5.3724449]) variable(0.08784976822950144)\n",
      "variable([[2.29793221]]) variable([5.37493575]) variable(0.08754029221162851)\n",
      "variable([[2.29308646]]) variable([5.37739285]) variable(0.08723906096725743)\n",
      "variable([[2.28830557]]) variable([5.37981674]) variable(0.08694585483964615)\n",
      "variable([[2.2835887]]) variable([5.38220792]) variable(0.0866604600272269)\n",
      "variable([[2.278935]]) variable([5.38456689]) variable(0.08638266842618712)\n",
      "variable([[2.27434366]]) variable([5.38689411]) variable(0.0861122774778659)\n",
      "variable([[2.26981384]]) variable([5.38919004]) variable(0.08584909002056745)\n",
      "variable([[2.26534474]]) variable([5.39145512]) variable(0.08559291414552149)\n",
      "variable([[2.26093555]]) variable([5.39368978]) variable(0.08534356305679344)\n",
      "variable([[2.25658547]]) variable([5.39589443]) variable(0.08510085493499035)\n",
      "variable([[2.25229371]]) variable([5.39806949]) variable(0.08486461280463413)\n",
      "variable([[2.2480595]]) variable([5.40021536]) variable(0.08463466440508784)\n",
      "variable([[2.24388206]]) variable([5.40233244]) variable(0.08441084206493275)\n",
      "variable([[2.23976064]]) variable([5.40442111]) variable(0.08419298257969864)\n",
      "variable([[2.23569448]]) variable([5.40648177]) variable(0.08398092709285435)\n",
      "variable([[2.23168285]]) variable([5.40851479]) variable(0.08377452097997208)\n",
      "variable([[2.22772501]]) variable([5.41052054]) variable(0.08357361373597812)\n",
      "variable([[2.22382025]]) variable([5.41249938]) variable(0.08337805886540826)\n",
      "variable([[2.21996785]]) variable([5.41445169]) variable(0.08318771377558704)\n",
      "variable([[2.21616712]]) variable([5.41637782]) variable(0.08300243967265336)\n",
      "variable([[2.21241735]]) variable([5.41827811]) variable(0.08282210146035615)\n",
      "variable([[2.20871786]]) variable([5.42015291]) variable(0.08264656764154595)\n",
      "variable([[2.20506799]]) variable([5.42200258]) variable(0.08247571022229121)\n",
      "variable([[2.20146706]]) variable([5.42382744]) variable(0.08230940461854906)\n",
      "variable([[2.19791443]]) variable([5.42562782]) variable(0.08214752956532231)\n",
      "variable([[2.19440943]]) variable([5.42740407]) variable(0.08198996702823673)\n",
      "variable([[2.19095144]]) variable([5.42915649]) variable(0.08183660211747391)\n",
      "variable([[2.18753983]]) variable([5.43088541]) variable(0.08168732300399718)\n",
      "variable([[2.18417396]]) variable([5.43259114]) variable(0.08154202083800904)\n",
      "variable([[2.18085323]]) variable([5.434274]) variable(0.08140058966958151)\n",
      "variable([[2.17757703]]) variable([5.4359343]) variable(0.08126292637140048)\n",
      "variable([[2.17434477]]) variable([5.43757232]) variable(0.0811289305635685)\n",
      "variable([[2.17115586]]) variable([5.43918838]) variable(0.08099850454041051)\n",
      "variable([[2.16800971]]) variable([5.44078277]) variable(0.0808715531992303)\n",
      "variable([[2.16490575]]) variable([5.44235578]) variable(0.08074798397096412)\n",
      "variable([[2.16184341]]) variable([5.44390769]) variable(0.08062770675268231)\n",
      "variable([[2.15882214]]) variable([5.44543879]) variable(0.08051063384188899)\n",
      "variable([[2.15584139]]) variable([5.44694936]) variable(0.08039667987257197)\n",
      "variable([[2.15290062]]) variable([5.44843967]) variable(0.08028576175295649)\n",
      "variable([[2.14999928]]) variable([5.44990999]) variable(0.08017779860491725)\n",
      "variable([[2.14713684]]) variable([5.4513606]) variable(0.08007271170500452)\n",
      "variable([[2.1443128]]) variable([5.45279175]) variable(0.07997042442704135)\n",
      "variable([[2.14152662]]) variable([5.45420371]) variable(0.07987086218625004)\n",
      "variable([[2.13877781]]) variable([5.45559674]) variable(0.07977395238486742)\n",
      "variable([[2.13606587]]) variable([5.45697108]) variable(0.07967962435920853)\n",
      "variable([[2.13339029]]) variable([5.458327]) variable(0.07958780932814088)\n",
      "variable([[2.13075059]]) variable([5.45966473]) variable(0.07949844034293135)\n",
      "variable([[2.12814629]]) variable([5.46098452]) variable(0.07941145223842926)\n",
      "variable([[2.12557692]]) variable([5.46228661]) variable(0.07932678158554966)\n",
      "variable([[2.123042]]) variable([5.46357124]) variable(0.07924436664502324)\n",
      "variable([[2.12054108]]) variable([5.46483864]) variable(0.07916414732237737)\n",
      "variable([[2.11807369]]) variable([5.46608905]) variable(0.07908606512411756)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = 5 + 2*x + np.random.rand(100,1)\n",
    "\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "W = Variable(np.zeros((1,1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "    y = matmul(x, W) + b\n",
    "    return y\n",
    "\n",
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    W.cleargrad()\n",
    "    b.cleargrad()\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data -= lr * W.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "    \n",
    "    print(W,b,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[ 0.00693193 -0.00160583 -0.00140474  0.01070093 -0.01124242 -0.00739062\n",
      "           -0.00395771  0.00091413 -0.00043655 -0.0026565 ]]) variable(0.8709474113416129)\n",
      "variable([[-0.64077155 -1.03587071 -0.72578173 -0.0678613  -1.63545598 -0.93475781\n",
      "           -0.63270238 -0.81444226 -0.95607165 -2.17288875]]) variable(0.26774291816856405)\n",
      "variable([[-0.57115267 -0.92980266 -0.64104352 -0.18421192 -1.66433439 -0.82999593\n",
      "           -0.56479673 -0.71823363 -0.85056993 -2.98371048]]) variable(0.2602933661712346)\n",
      "variable([[-0.49705276 -0.7632952  -0.54303209 -0.3257606  -1.48460807 -0.68277288\n",
      "           -0.49329052 -0.59763061 -0.69895922 -4.54845234]]) variable(0.23650229278109183)\n",
      "variable([[-0.53810505 -0.89373832 -0.59811249 -0.49499416 -1.68524225 -0.78882893\n",
      "           -0.53421691 -0.67250412 -0.81010736 -6.73330914]]) variable(0.18630482333010762)\n",
      "variable([[-0.78959745 -1.48006198 -0.92891674 -0.78066503 -2.44739261 -1.30579457\n",
      "           -0.780697   -1.08688537 -1.34258275 -8.90200081]]) variable(0.111940896328927)\n",
      "variable([[-0.98013123 -1.77703258 -1.1594253  -0.97863471 -2.65073872 -1.59320087\n",
      "           -0.96825102 -1.34865876 -1.63271453 -9.85438824]]) variable(0.08866492413172451)\n",
      "variable([[-1.04990035 -1.82906706 -1.23464834 -1.05031792 -2.56334619 -1.65777668\n",
      "           -1.03739899 -1.42278027 -1.69498185 -9.9332419 ]]) variable(0.08659689233883673)\n",
      "variable([[-1.08775472 -1.83987534 -1.27247251 -1.08908357 -2.47370605 -1.68024883\n",
      "           -1.07507565 -1.45613924 -1.71520216 -9.81651604]]) variable(0.08589543606677749)\n",
      "variable([[-1.11742107 -1.84880913 -1.30197638 -1.11940591 -2.4162497  -1.69804253\n",
      "           -1.10461401 -1.48217915 -1.73128954 -9.67242185]]) variable(0.0853931741865887)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100,1)\n",
    "y = np.sin(2*np.pi*x) + np.random.rand(100,1)\n",
    "\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "I, H, K = 1, 10, 1\n",
    "W1 = Variable(0.01 * np.random.randn(I,H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "W2 = Variable(0.01 * np.random.randn(H,K))\n",
    "b2 = Variable(np.zeros(K))\n",
    "\n",
    "def predict(x):\n",
    "    y = linear(x, W1, b1)\n",
    "    y = sigmoid(y)\n",
    "    y = linear(y, W2, b2)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = mean_squared_error(y,y_pred)\n",
    "    \n",
    "    W1.cleargrad()\n",
    "    b1.cleargrad()\n",
    "    W2.cleargrad()\n",
    "    b2.cleargrad()\n",
    "    loss.backward()\n",
    "    \n",
    "    W1.data -= lr*W1.grad.data\n",
    "    b1.data -= lr*b1.grad.data\n",
    "    W2.data -= lr*W2.grad.data\n",
    "    b2.data -= lr*b2.grad.data\n",
    "    if i%1000==0:\n",
    "        print(W1,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'cleargrad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a02ed01d226b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-5e047bd5fa32>\u001b[0m in \u001b[0;36mcleargrads\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcleargrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleargrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'cleargrad'"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = np.sin(2*np.pi*x) + np.random.rand(100,1)\n",
    "\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size,1))\n",
    "optimizer = SGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update()\n",
    "    if i%1000==0:\n",
    "        print(y_pred)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
